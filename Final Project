{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KeyboardNotFound_SVR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "9aead3e0f37a9fbddbdeea0b721371908ae27e6c371bbfa1bdc57644ea526b8a"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF3aygGzwh3Z"
      },
      "source": [
        "# Reference Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n32PvoBSBnf4"
      },
      "source": [
        "* Kaggle: https://www.kaggle.com/c/bigquery-geotab-intersection-congestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuiGM9ogwpM7"
      },
      "source": [
        "#Data Set Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1cgcgJVDM7c"
      },
      "source": [
        "The data consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks. The data have been grouped by intersection, month, hour of day, direction driven through the intersection, and whether the day was on a weekend or not.\n",
        "\n",
        "For each grouping in the test set, you need to make predictions for three different quantiles of two different metrics covering how long it took the group of vehicles to drive through the intersection. Specifically, the 20th, 50th, and 80th percentiles for the total time stopped at an intersection and the distance between the intersection and the first place a vehicle stopped while waiting. You can think of your goal as summarizing the distribution of wait times and stop distances at each intersection.\n",
        "\n",
        "Each of those six predictions goes on a new row in the submission file. Read the submission TargetId fields, such as 1_1, as the first number being the RowId and the second being the metric id. You can unpack the submission metric id codes with submission_metric_map.json.\n",
        "\n",
        "The training set includes an optional additional output metric (TimeFromFirstStop) in case you find that useful for building your models. It was only excluded from the test set to limit the number of predictions that must be made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CegmXfpvzA0"
      },
      "source": [
        "# Importing Data Set and Evironment Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL3QDNCuTnUD"
      },
      "source": [
        "#upload kaggle authorization credentials\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vISg0XtDUER4"
      },
      "source": [
        " ! pip install kaggle==1.5.6\n",
        " ! mkdir ~/.kaggle\n",
        " ! cp kaggle.json ~/.kaggle/\n",
        " ! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSebOWW-UVR2"
      },
      "source": [
        "! mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_q8yeyUkUz"
      },
      "source": [
        "! kaggle competitions download -c bigquery-geotab-intersection-congestion -p /content/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0EH1X47qwKi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xxd7T8ZVsw_"
      },
      "source": [
        "! unzip /content/dataset/bigquery-geotab-intersection-congestion.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvKmopjQvWJi"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA1tnco3V_vL"
      },
      "source": [
        "! head train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EPw1JaNoEfH"
      },
      "source": [
        "! tail train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2rzWubqWeVg"
      },
      "source": [
        "! cat train.csv | wc -l \n",
        "! cat test.csv | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhB0xOE3sB7z"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI-38pCfsdGz"
      },
      "source": [
        "train_data = pd.read_csv('./train.csv')#, index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB4M4FT7wFSw"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDLGZX6hvgxy"
      },
      "source": [
        "train_data.groupby(['City']).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvfRwBnKWGDB"
      },
      "source": [
        "test_data = pd.read_csv('./test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjH-7yf9uvT8"
      },
      "source": [
        "## Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDGxhRikuy1m"
      },
      "source": [
        "any(train_data.isna())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJAZw-6au3XM"
      },
      "source": [
        "#function for identfying which columns have missing values\n",
        "def identify_nan(dataframe):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for i in dataframe.columns.to_list():\n",
        "\n",
        "        result.update({i: dataframe[i].isnull().values.any()})\n",
        "\n",
        "    return result\n",
        "\n",
        "#variable to store the above function result\n",
        "identify_columns_nan = identify_nan(train_data)\n",
        "print(identify_columns_nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z_G85mzu5bG"
      },
      "source": [
        "#which columns have at least one missing value\n",
        "identify_output_nan = {}\n",
        "\n",
        "for key, value in identify_columns_nan.items():\n",
        "    \n",
        "    if (value == True):\n",
        "\n",
        "        identify_output_nan[key] = value\n",
        "\n",
        "print(identify_output_nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9RMwPZQu-Ic"
      },
      "source": [
        "is_NaN = train_data.isnull()\n",
        "row_has_NaN = is_NaN.any(axis=1)\n",
        "rows_with_NaN = train_data[row_has_NaN]\n",
        "print(rows_with_NaN.shape) #how many rows with at least 1 NaN value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Se-e1Abx_mq"
      },
      "source": [
        "##Data Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33yijczIxcrH"
      },
      "source": [
        "train_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czqs2AFuOBp9"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFP4vDUIPj3u"
      },
      "source": [
        "pip install haversine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ciIC5b5OFZG"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "dataset = train_data\n",
        "\n",
        "df['City'] = dataset['City']\n",
        "df['Latitude'] = dataset['Latitude']\n",
        "df['Longitude'] = dataset['Longitude']\n",
        "df['Weekend'] = dataset.apply(lambda x: 1-x['Weekend'], axis=1) #value -1 in order to consider negative correlation\n",
        "df.head()\n",
        "\n",
        "df_test = pd.DataFrame()\n",
        "dataset_test = test_data\n",
        "df_test['City'] = dataset_test['City']\n",
        "df_test['Latitude'] = dataset_test['Latitude']\n",
        "df_test['Longitude'] = dataset_test['Longitude']\n",
        "df_test['Weekend'] = dataset_test.apply(lambda x: 1-x['Weekend'], axis=1) #value -1 in order to consider negative correlation\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwmEVZ-HHo24"
      },
      "source": [
        "##Rush Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl1rNRN1OQtt"
      },
      "source": [
        "def rush_hour(X):\n",
        "    hour = X['Hour']\n",
        "    if (hour >= 8 and hour < 10) or (hour >= 18 and hour < 20):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z3mcGO1ORPk"
      },
      "source": [
        "df['RushHour'] = dataset.apply(rush_hour, axis=1)\n",
        "df['RushHour']\n",
        "df_test['RushHour'] = dataset_test.apply(rush_hour, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-85dMJuICvH"
      },
      "source": [
        "##Time of the Day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii2qAiqMOSu8"
      },
      "source": [
        "def group_hour(X):\n",
        "  hour = X['Hour']\n",
        "  if hour > 4 and hour <= 8:\n",
        "    return 'early morning'\n",
        "  elif hour < 10:\n",
        "    return 'morning rush'\n",
        "  elif hour < 12:\n",
        "    return 'late morning'\n",
        "  elif hour < 18:\n",
        "    return 'afternoon'\n",
        "  elif hour < 20:\n",
        "    return 'night rush'\n",
        "  elif hour < 23:\n",
        "    return 'night'\n",
        "  else:\n",
        "    return 'late night'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKg9PAcIOWii"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "hourGrouped = dataset.apply(group_hour, axis=1)\n",
        "hourOHE = OneHotEncoder().fit_transform(hourGrouped.values.reshape(-1, 1))\n",
        "\n",
        "df = pd.concat([df, pd.DataFrame(hourOHE.toarray()\n",
        "                                , index=df.index\n",
        "                                , columns=['HourOHE1', 'HourOHE2', 'HourOHE3', 'HourOHE4', 'HourOHE5', 'HourOHE6', 'HourOHE7'])], axis=1)\n",
        "del(hourGrouped)\n",
        "del(hourOHE)\n",
        "\n",
        "df.head()\n",
        "\n",
        "hourGrouped = dataset_test.apply(group_hour, axis=1)\n",
        "hourOHE = OneHotEncoder().fit_transform(hourGrouped.values.reshape(-1, 1))\n",
        "df_test = pd.concat([df_test, pd.DataFrame(hourOHE.toarray()\n",
        "                                , index=df_test.index\n",
        "                                , columns=['HourOHE1', 'HourOHE2', 'HourOHE3', 'HourOHE4', 'HourOHE5', 'HourOHE6', 'HourOHE7'])], axis=1)\n",
        "del(hourGrouped)\n",
        "del(hourOHE)\n",
        "\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9QpdbFQOXDV"
      },
      "source": [
        "def normalize_coordinate(coord): #truncate coord in order to avoid overfitting\n",
        "    return int(coord*1000)  \n",
        "\n",
        "def heading_vector(heading):\n",
        "    (V, H) = (0, 0)\n",
        "    if 'N' in heading:\n",
        "        V = 1\n",
        "    if 'S' in heading:\n",
        "        V = -1\n",
        "    if 'W' in heading:\n",
        "        H = -1\n",
        "    if 'E' in heading:\n",
        "        H = 1\n",
        "    return (V, H)\n",
        "\n",
        "def heading_direction(X):\n",
        "    lt, lg, heading = normalize_coordinate(X['Latitude']), normalize_coordinate(X['Longitude']), X['ExitHeading']\n",
        "    ltv, lgv = heading_vector(heading)\n",
        "    return (ltv, lgv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zo-_qQObLr"
      },
      "source": [
        "df['HeadingX'] = dataset.apply(lambda x: heading_direction(x)[0], axis=1)\n",
        "df['HeadingY'] = dataset.apply(lambda x: heading_direction(x)[1], axis=1)\n",
        "df.head()\n",
        "\n",
        "df_test['HeadingX'] = dataset_test.apply(lambda x: heading_direction(x)[0], axis=1)\n",
        "df_test['HeadingY'] = dataset_test.apply(lambda x: heading_direction(x)[1], axis=1)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgtPrA4vOljC"
      },
      "source": [
        "def curve_abs(X):\n",
        "  entry, exit = X['EntryHeading'], X['ExitHeading']\n",
        "  (x_in, y_in) = heading_vector(entry)\n",
        "  (x_out, y_out) = heading_vector(exit)\n",
        "\n",
        "  return abs(x_out - x_in) + abs(y_out-y_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qiS-FgIOl-h"
      },
      "source": [
        "import numpy as np\n",
        "df['Curvature'] =  np.asarray(train_data.apply(curve_abs, axis=1)).astype('int')\n",
        "df['Curvature']\n",
        "\n",
        "df_test['Curvature'] =  np.asarray(test_data.apply(curve_abs, axis=1)).astype('int')\n",
        "df_test['Curvature']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iB46pKVOneE"
      },
      "source": [
        "from haversine import haversine\n",
        "\n",
        "cityCenterLocation ={'Atlanta':[33.753746, -84.386330], \n",
        "                      'Boston':[42.361145, -71.057083], \n",
        "                      'Chicago':[41.881832, -87.623177], \n",
        "                      'Philadelphia':[39.952583, -75.165222]}\n",
        "def getDistancePoint(X):\n",
        "  city, latPoint, lonPoint = X['City'], X['Latitude'], X['Longitude']\n",
        "  latCity, lonCity = cityCenterLocation[city]\n",
        "\n",
        "  return haversine((latCity, lonCity), (latPoint, lonPoint))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE4F_6bNO4Hl"
      },
      "source": [
        "df['Dist_Downtown'] = dataset.apply(getDistancePoint, axis=1)\n",
        "df['Dist_Downtown']\n",
        "\n",
        "df_test['Dist_Downtown'] = dataset_test.apply(getDistancePoint, axis=1)\n",
        "df_test['Dist_Downtown']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofQqpRxBOqID"
      },
      "source": [
        "def to_downtown(X): #depending if the path is towards downtown and in how many directions (0, 1 or 2)\n",
        "  city, latPoint, lonPoint = X['City'], X['Latitude'], X['Longitude']\n",
        "  cityCenterLocation ={'Atlanta':[33.753746, -84.386330], \n",
        "                      'Boston':[42.361145, -71.057083], \n",
        "                      'Chicago':[41.881832, -87.623177], \n",
        "                      'Philadelphia':[39.952583, -75.165222]}\n",
        "\n",
        "  latCity, lonCity = cityCenterLocation[city]\n",
        "  epsilon = 0.0001\n",
        "  dx, dy = np.array(heading_vector(X['ExitHeading'])) * epsilon\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  if abs(latPoint+dx - latCity) < abs(latPoint - latCity):\n",
        "      score += 1 # aproxima latitude\n",
        "\n",
        "  if abs(lonPoint+dy - lonCity) < abs(lonPoint - lonCity):\n",
        "      score += 1 # aproxima longitude\n",
        "\n",
        "  return score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnyjxP59OyAn"
      },
      "source": [
        "df['ToDowntown']=dataset.apply(to_downtown, axis=1)\n",
        "df['ToDowntown']\n",
        "\n",
        "df_test['ToDowntown']=dataset_test.apply(to_downtown, axis=1)\n",
        "df_test['ToDowntown']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAJUSGNXPG2c"
      },
      "source": [
        "city_month = dataset[\"City\"].astype(str) + dataset[\"Month\"].astype(str)\n",
        "city_month = dataset_test[\"City\"].astype(str) + dataset_test[\"Month\"].astype(str)\n",
        "\n",
        "monthly_temp = {'Atlanta1': 43, 'Atlanta5': 69, 'Atlanta6': 76, 'Atlanta7': 79, 'Atlanta8': 78, \n",
        "                'Atlanta9': 73, 'Atlanta10': 62, 'Atlanta11': 53, 'Atlanta12': 45, 'Boston1': 30, \n",
        "                'Boston5': 59, 'Boston6': 68, 'Boston7': 74, 'Boston8': 73, 'Boston9': 66, \n",
        "                'Boston10': 55,'Boston11': 45, 'Boston12': 35, 'Chicago1': 27, 'Chicago5': 60, \n",
        "                'Chicago6': 70, 'Chicago7': 76, 'Chicago8': 76, 'Chicago9': 68, \n",
        "                'Chicago10': 56,  'Chicago11': 45, 'Chicago12': 32, 'Philadelphia1': 35, \n",
        "                'Philadelphia5': 66, 'Philadelphia6': 76, 'Philadelphia7': 81, \n",
        "                'Philadelphia8': 79, 'Philadelphia9': 72, 'Philadelphia10': 60, \n",
        "                'Philadelphia11': 49, 'Philadelphia12': 40}\n",
        "\n",
        "df['TempAvg'] = city_month.map(monthly_temp)\n",
        "df_test['TempAvg'] = city_month.map(monthly_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw1-tnmHPHXf"
      },
      "source": [
        "monthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, \n",
        "                    'Atlanta8': 3.67, 'Atlanta9': 4.09, 'Atlanta10': 3.11, 'Atlanta11': 4.10, \n",
        "                    'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22, \n",
        "                    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79, \n",
        "                    'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38, \n",
        "                    'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27, \n",
        "                    'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, \n",
        "                    'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n",
        "                    'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 , \n",
        "                    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n",
        "\n",
        "df['RainAvg'] = city_month.map(monthly_rainfall)\n",
        "df_test['RainAvg'] = city_month.map(monthly_rainfall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hnry45O8iX"
      },
      "source": [
        "## Data Preparation and Results Treatment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q07yTy0oPBSw"
      },
      "source": [
        "def register_result(city, target, rmse, tag):\n",
        "  \n",
        "  (rmse_res, tag_res) = best_values.get((city, target), (99999, 'default'))\n",
        "  if(rmse_res > rmse):\n",
        "    best_values[(city, target)] = (rmse, tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98E_IjFVVczs"
      },
      "source": [
        "def make_city_dict(df):\n",
        "  train_data_dict = {\n",
        "      \"Atlanta\": df[df[\"City\"] == \"Atlanta\"],\n",
        "      \"Boston\": df[df[\"City\"] == \"Boston\"],\n",
        "      \"Chicago\": df[df[\"City\"] == \"Chicago\"],\n",
        "      \"Philadelphia\": df[df[\"City\"] == \"Philadelphia\"]\n",
        "  }\n",
        "  return train_data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXousTK9PPmx"
      },
      "source": [
        "targets = ['TotalTimeStopped_p20',\n",
        "           'TotalTimeStopped_p50',\n",
        "           'TotalTimeStopped_p80',           \n",
        "           'DistanceToFirstStop_p20',\n",
        "           'DistanceToFirstStop_p50',\n",
        "           'DistanceToFirstStop_p80']\n",
        "\n",
        "pseudo_targets = ['TimeFromFirstStop_p20',\n",
        "                  'TotalTimeStopped_p40',\n",
        "                  'TotalTimeStopped_p60',\n",
        "                  'DistanceToFirstStop_p40',\n",
        "                  'DistanceToFirstStop_p60',\n",
        "                  'TimeFromFirstStop_p40',\n",
        "                  'TimeFromFirstStop_p50',\n",
        "                  'TimeFromFirstStop_p60',\n",
        "                  'TimeFromFirstStop_p80']\n",
        "\n",
        "non_targets = ['City', 'Latitude', 'Longitude']\n",
        "\n",
        "best_values = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H9noBiRPSkN"
      },
      "source": [
        "for target in targets:\n",
        "    df[target] = dataset[target]\n",
        "\n",
        "def generate_features():    \n",
        "  featuresSet = set(df.columns) - set(targets) \n",
        "  featuresSet = featuresSet - set(non_targets)\n",
        "  featuresSet = featuresSet - set(pseudo_targets)\n",
        "\n",
        "  features = list(featuresSet)\n",
        "  return features\n",
        "\n",
        "features = generate_features()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mbWKp2qPcE-",
        "tags": []
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "targets_plot = targets\n",
        "\n",
        "dfInteresse = pd.concat([df[features], df[targets_plot]], axis=1)\n",
        "\n",
        "fig = plt.subplots(figsize=(20,20))\n",
        "sns.heatmap(dfInteresse.corr(), vmax=1, square=True, annot=True, cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHatR0_bTVKw"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "train_data['PathOHE'] = encoder.fit_transform(train_data['Path'])\n",
        "train_data['ExitHeadingOHE']     = encoder.fit_transform(train_data['ExitHeading'])\n",
        "train_data['EntryHeadingOHE']    = encoder.fit_transform(train_data['EntryHeading'])\n",
        "train_data['CityOHE']            = encoder.fit_transform(train_data['City'])\n",
        "\n",
        "test_data['PathOHE'] = encoder.fit_transform(test_data['Path'])\n",
        "test_data['ExitHeadingOHE']     = encoder.fit_transform(test_data['ExitHeading'])\n",
        "test_data['EntryHeadingOHE']    = encoder.fit_transform(test_data['EntryHeading'])\n",
        "test_data['CityOHE']            = encoder.fit_transform(test_data['City'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqSC7QjsucBO"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOZt8cOaHtEq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWRico_XvkzB"
      },
      "source": [
        "correlation = train_data.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT3yLJmaIgSy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "fig = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='Blues')\n",
        "features = ['Latitude', 'Longitude', 'EntryHeadingOHE', 'ExitHeadingOHE', 'CityOHE', 'Hour', 'Weekend', 'Month']\n",
        "x = train_data[features]\n",
        "y_time = train_data['TotalTimeStopped_p80']\n",
        "y_dist = train_data['DistanceToFirstStop_p80']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtL0zxzvSmFW"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, x_val, y_train, y_val) = train_test_split(x, y_time, test_size = .25)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "predict = regressor.predict(x_val)\n",
        "accuracy = regressor.score(x_val, y_val)\n",
        "\n",
        "print(\"MSE:\\t\\t\", mean_squared_error(y_val, predict))\n",
        "print(\"MAE:\\t\\t\", mean_absolute_error(y_val, predict))\n",
        "print(\"R2 score:\\t\",accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufLRCBRGYHda"
      },
      "source": [
        "(x_train, x_val, y_train, y_val) = train_test_split(x, y_dist, test_size = .25)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "predict = regressor.predict(x_val)\n",
        "accuracy = regressor.score(x_val, y_val)\n",
        "\n",
        "print(\"MSE:\\t\\t\", mean_squared_error(y_val, predict))\n",
        "print(\"MAE:\\t\\t\", mean_absolute_error(y_val, predict))\n",
        "print(\"R2 score:\\t\",accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV8_BnXaRXtE"
      },
      "source": [
        "# ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0HuSnncRZgB"
      },
      "source": [
        "## SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zznP13btRuWM"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vqL8FA8drco"
      },
      "source": [
        "###Reduzir Complexidade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMKaXesjAYp"
      },
      "source": [
        "####Features Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5W3KtSbfNMX"
      },
      "source": [
        "dfAtlanta = df[df['City'] == 'Atlanta'].drop('City', axis = 1)\n",
        "dfBoston = df[df['City'] == 'Boston'].drop('City', axis = 1)\n",
        "dfChicago = df[df['City'] == 'Chicago'].drop('City', axis = 1)\n",
        "dfPhiladelphia = df[df['City'] == 'Philadelphia'].drop('City', axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp2e5USfTXhQ"
      },
      "source": [
        "cities_list = [[dfAtlanta, 'Atlanta'],\n",
        "               [dfBoston, 'Boston'],\n",
        "               [dfChicago, 'Chicago'],\n",
        "               [dfPhiladelphia ,'Philadelphia']\n",
        "              ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzmlAozqVWPl"
      },
      "source": [
        "###Hyperparameters Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqS2pMa2xPf_"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WKcIMv_umU5"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "kernel_types = ['linear','rbf']\n",
        "C_range = np.logspace(-2, 10, 2, base=2)\n",
        "gamma_range = np.logspace(-9, 3, 2, base=2)\n",
        "epsilon_range = np.logspace(-2, 10, 2, base=2)\n",
        "\n",
        "k_features = 2 # of features\n",
        "\n",
        "dfParametersToPredict = pd.DataFrame(columns=['City','Target','Feature1', 'Feature2','GS_BestParameters', 'x_fit_dataset', 'y_fit_dataset', 'model_fit'])\n",
        "\n",
        "dictMapeamento = {}\n",
        "\n",
        "count = 0\n",
        "\n",
        "for t in targets:\n",
        "\n",
        "  print(t,'\\n')\n",
        "\n",
        "  for city,ref in cities_list:\n",
        "\n",
        "    exclude_list = list(set(city[targets].columns.to_list()) - set([t])) \n",
        "\n",
        "    dfAlvo = city.drop(exclude_list, axis=1)\n",
        "\n",
        "    # Create and fit selector\n",
        "    selector = SelectKBest(score_func=f_regression, k=k_features)\n",
        "    selector.fit(dfAlvo.drop([t], axis = 1), dfAlvo[[t]])\n",
        "    # Get columns to keep and create new dataframe with those only\n",
        "    cols = selector.get_support(indices=True)\n",
        "\n",
        "    features_df_new = dfAlvo.iloc[:,cols]\n",
        "    featuresSvr = list(features_df_new.columns)\n",
        "    dfFinal=pd.concat([features_df_new, dfAlvo[[t]]], axis=1)\n",
        "    \n",
        "    train_summarized = dfFinal.groupby(featuresSvr, as_index=False)\\\n",
        "                                            .agg({t:[np.mean]})\n",
        "\n",
        "    print('\\t',ref,'\\n\\t','#Entries:',len(train_summarized),'\\n\\t','Best',k_features,'Features:',featuresSvr,'\\n')\n",
        "\n",
        "    param_dist = dict(C = C_range, epsilon = epsilon_range, gamma = gamma_range)\n",
        "\n",
        "    #grid search\n",
        "    for kernel in kernel_types:\n",
        "        grid = GridSearchCV(estimator = SVR(kernel = kernel),\n",
        "                                    param_grid = param_dist,\n",
        "                                    cv = 3,\n",
        "                                    n_jobs = -1,\n",
        "                                    scoring = 'neg_mean_absolute_error')\n",
        "\n",
        "        grid.fit(train_summarized[featuresSvr], train_summarized[[t]].values.ravel())\n",
        "\n",
        "        print('\\n\\t Kernel {} | Parametros: {} | Score: {}'.format(kernel, grid.best_params_, grid.best_score_))\n",
        "\n",
        "        sv = grid.best_estimator_\n",
        "        model_fit_variable = sv.fit(train_summarized[featuresSvr], train_summarized[[t]].values.ravel())\n",
        "        \n",
        "    dictMapeamento[(ref,t)] = count\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    listParameters = [ref, \n",
        "                      t, \n",
        "                      featuresSvr[0], \n",
        "                      featuresSvr[1], \n",
        "                      grid.best_params_, \n",
        "                      train_summarized[featuresSvr], \n",
        "                      train_summarized[[t]].values.ravel(),\n",
        "                      model_fit_variable]\n",
        "\n",
        "    dfParametersToPredict.loc[len(dfParametersToPredict)] = listParameters\n",
        "\n",
        "    print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqI6138ssWRt"
      },
      "source": [
        "##Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5ABCpoFMcd"
      },
      "source": [
        "dfParametersToPredict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2hNK-n7fjXr"
      },
      "source": [
        "firstId = 0\n",
        "secondId = 0\n",
        "\n",
        "for index, row in df_test[:].iterrows():\n",
        "    \n",
        "    secondId = 0\n",
        "\n",
        "    for i in targets:\n",
        "\n",
        "      city = row['City']\n",
        "\n",
        "      primeiro_parametro = dfParametersToPredict.loc[(dfParametersToPredict['City'] == city) &\n",
        "                                                     (dfParametersToPredict['Target'] == i), 'Feature1']\n",
        "      \n",
        "      segundo_parametro = dfParametersToPredict.loc[(dfParametersToPredict['City'] == city) &\n",
        "                                                    (dfParametersToPredict['Target'] == i), 'Feature2']\n",
        "\n",
        "      feature1, feature2 = (row[primeiro_parametro].values, row[segundo_parametro].values)\n",
        "      print(city, feature1, feature2)\n",
        "\n",
        "      iteration_bestEstimator = dfParametersToPredict.loc[(dfParametersToPredict['City'] == city) &\n",
        "                                                          (dfParametersToPredict['Target'] == i), :]['GS_BestParameters']\n",
        "\n",
        "      print(primeiro_parametro.values,'\\n', segundo_parametro.values)\n",
        "\n",
        "      sv = SVR(iteration_bestEstimator)\n",
        "\n",
        "      print('\\n',i, city)\n",
        "\n",
        "      model_fit = dfParametersToPredict.loc[(dfParametersToPredict['City'] == city) &\n",
        "                                                     (dfParametersToPredict['Target'] == i), :]['model_fit'][dictMapeamento[(city,i)]]\n",
        "\n",
        "      y_pred = model_fit.predict(pd.concat([row[primeiro_parametro].to_frame().T,row[segundo_parametro].to_frame().T], axis = 1))\n",
        "      print(y_pred)\n",
        "\n",
        "      print('{}_{}'.format(firstId,secondId))\n",
        "\n",
        "      secondId = secondId + 1\n",
        "      \n",
        "      print('\\n')\n",
        "\n",
        "    firstId = firstId + 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2YGsPvkwUuh"
      },
      "source": [
        "#Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNvS9wx5wZJa"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
